{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Homework 2** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Classification**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Divya** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 23/4** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Oskar Hulthén, 950801-1195, huoskar@student.chalmers** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                  ** And: Alexander Branzell, 931003-1977, alebra@student.chalmers.se** <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General guidelines:\n",
    "* All solutions to theoretical problems, can be submitted as a single file named *report.pdf*. They can also be submitted in this ipynb notebook, but equations wherever required, should be formatted using LaTeX math-mode.\n",
    "* All discussion regarding practical problems, along with solutions and plots should be specified here itself. We will not generate the solutions/plots again by running your code.\n",
    "* Your name, personal number and email address should be specified above and also in your file *report.pdf*.\n",
    "* All datasets can be downloaded from the course website.\n",
    "* All tables and other additional information should be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical problems\n",
    "\n",
    "## [Naive Bayes Classifier, 6 points]\n",
    "\n",
    "A psychologist does a small survey on ''happiness''. Each respondent provides a vector with entries 1 or 0 corresponding to if they answered “yes” or “no” to a question respectively. The question vector has attributes \n",
    "$$\n",
    "x = (\\mbox{rich, married, healthy}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Thus a response $(1, 0, 1)$ would indicate that the respondent was\n",
    "''rich'', ''unmarried'' and ''healthy''. In addition, each respondent\n",
    "gives a value $c = 1$ if they are content wih their life and $c = 0$\n",
    "if they’re not. The following responses were obtained.\n",
    "\n",
    "$$\n",
    "c = 1: (1, 1, 1),(0, 0, 1),(1, 1, 0),(1, 0, 1) \\\\\n",
    "c = 0: (0, 0, 0),(1, 0, 0),(0, 0, 1),(0, 1, 0)\n",
    "$$\n",
    "\n",
    "1. Using naive Bayes, what is the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content''?\n",
    "\n",
    "2. What is the probability that a person who is ''not rich'' and ''married'' is content ? (i.e. we do not know if they are ''healthy'')\n",
    "\n",
    "## [Extending Naive Bayes, 4 points]\n",
    "\n",
    "Consider now, the following vector of attributes:\n",
    "\n",
    "* $x_1 = 1$ if customer is younger than 20 and 0 otherwise.\n",
    "* $x_2 = 1$ if customer is between 20 and 30 in age, and 0 otherwise.\n",
    "* $x_3 = 1$ if customer is older than 30 and 0 otherwise\n",
    "* $x_4 = 1$ if customer walks to work and 0 otherwise.\n",
    "\n",
    "Each vector of attributes has a label ''rich'' or ''poor''. Point out potential difficulties with your approach above to training using naive Bayes. Suggest and describe how to extend your naive Bayes method to this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers\n",
    "For answers to the theoretical questions see the pdf report attached together with this notebook file on hand-in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical problems\n",
    "\n",
    "## [Bayes classifier, 5 points]\n",
    "\n",
    "Dowload the dataset **\"dataset2.txt\"**. You can use the following code for example:\n",
    "```python\n",
    "from numpy import genfromtxt\n",
    "data = genfromtxt('dataset2.txt', delimiter=',')\n",
    "labels = data[:,-1]\n",
    "```\n",
    "The dataset contains $3$-dimensional data, $X$, generated from $2$ classes with labels, $y$ either $+1$ or $-1$.  Each row of $X$ and $y$ contain one observation and one label respectively.  There are $1000$ instances of each class. \n",
    "\n",
    "a. Assume that the class conditional density is spherical Gaussian, and both classes have equal prior. Write the expression for the Bayes (<span style=\"color:red\"> not **naive Bayes**</span>) classifier i.e. derive\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) \\\\\n",
    "P(y_{new} = +1 | x_{new} , X, y ) ~.\n",
    "$$\n",
    "\n",
    "It is useful to note that the dependence on training data $X, y$ for class $1$ can be expressed as: \n",
    "\n",
    "$$ \n",
    "P( x_{new} | y_{new} = 1, X, y) = P(x_{new} |\n",
    "\\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1})\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mu}_{1} \\in \\mathbb{R}^3$ and $\\hat{\\sigma}^{2}_{1}\\in \\mathbb{R}$ are MLE estimates for mean (3-dimensional) and variance based on training data with label $+1$ (and similarly for class 2 with label $-1$). \n",
    "\n",
    "b. Implement a function **sph_bayes()** which computes the probability of a new test point *Xtest* coming from class $1$ ($P1$) and class $2$ ($P2$). Finally, assign a label *Ytest* to the test point based on the probabilities $P1$ and $P2$.\n",
    "\n",
    "```python\n",
    "def sph_bayes(Xtest, ...): # other parameters needed.\n",
    "\n",
    "    return [P1, P2, Ytest]\n",
    "```\n",
    "c. Write a function **new_classifier()**\n",
    "\n",
    "```python\n",
    "def new_classifier(Xtest, mu1, mu2)\n",
    "    \n",
    "    return [Ytest]\n",
    "```\n",
    "which implements the following classifier,\n",
    "$$\n",
    "f(x) = \\mbox{sign}\\left(\\frac{(\\mu_1 - \\mu_2)^\\top (x - b) }{\\|\\mu_1 -  \\mu_2\\|_2} \\right)\n",
    "$$\n",
    "with $b = \\frac{1}{2}(\\mu_1 + \\mu_2)$.\n",
    "\n",
    "d. Report 5-fold cross validation error for both classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers:\n",
    "## a)\n",
    "Following the Bayes formula for prediction:\n",
    "$$P(t_{new}=k\\ |\\ x_{new},\\mathbf{X},\\mathbf{t}) = \\frac{p(x_{new}\\ |\\ t_{new}=k,\\mathbf{X},\\mathbf{t})P(t_{new}=k)}{\\sum _{j}p(x_{new}\\ |\\ t_{new}=j,\\mathbf{X},\\mathbf{t})P(t_{new}=j)}$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Where we have two classes $y_{new}=-1$ and $y_{new}=+1$. To predict the +1 class we get the following:\n",
    "\n",
    "$$P(y_{new}=+1\\ |\\ x_{new},\\mathbf{X},\\mathbf{t}) = \\frac{p(x_{new}\\ |\\ y_{new}=+1,\\mathbf{X},\\mathbf{t})P(y_{new}=+1)}{p(x_{new}\\ |\\ y_{new}=+1,\\mathbf{X},\\mathbf{t})P(y_{new}=+1) + p(x_{new}\\ |\\ y_{new}=-1,\\mathbf{X},\\mathbf{t})P(y_{new}=-1)}$$\n",
    "<br>\n",
    "<br>\n",
    "Given in the task is that both classes have equal prior distribution ($P(y_{new}=-1) = P(y_{new}=+1)$), meaning that the prior distribution can be removed:\n",
    "<br>\n",
    "\n",
    "$$P(y_{new}=+1\\ |\\ x_{new},\\mathbf{X},\\mathbf{t}) = \\frac{p(x_{new}\\ |\\ y_{new}=+1,\\mathbf{X},\\mathbf{t})}{p(x_{new}\\ |\\ y_{new}=+1,\\mathbf{X},\\mathbf{t}) + p(x_{new}\\ |\\ y_{new}=-1,\\mathbf{X},\\mathbf{t})}$$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "Furthermore we're given the way to express dependence upon the training data as $P( x_{new}\\ |\\ y_{new} = 1, X, y) = P(x_{new} | \\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1})$ yielding the following final expressions for Bayes classifier:\n",
    "<br>\n",
    "\n",
    "$$P(y_{new}=+1\\ |\\ x_{new},\\mathbf{X},\\mathbf{t}) = \\frac{P(x_{new}\\ |\\ \\hat{\\mu}_{+1}, \\hat{\\sigma}^{2}_{+1})}{P(x_{new}\\ |\\ \\hat{\\mu}_{+1}, \\hat{\\sigma}^{2}_{+1}) + P(x_{new}\\ |\\ \\hat{\\mu}_{-1}, \\hat{\\sigma}^{2}_{-1})}$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "$$P(y_{new}=-1\\ |\\ x_{new},\\mathbf{X},\\mathbf{t}) = \\frac{P(x_{new}\\ |\\ \\hat{\\mu}_{-1}, \\hat{\\sigma}^{2}_{-1})}{P(x_{new}\\ |\\ \\hat{\\mu}_{+1}, \\hat{\\sigma}^{2}_{+1}) + P(x_{new}\\ |\\ \\hat{\\mu}_{-1}, \\hat{\\sigma}^{2}_{-1})}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== 5-fold cross validation ======\n",
      "\n",
      "\n",
      "==== Iteration 1 ====\n",
      "Error of spherical bayes: 0.50 % \n",
      "Error of new classifier: 25.50 %\n",
      "\n",
      "\n",
      "==== Iteration 2 ====\n",
      "Error of spherical bayes: 0.75 % \n",
      "Error of new classifier: 23.75 %\n",
      "\n",
      "\n",
      "==== Iteration 3 ====\n",
      "Error of spherical bayes: 0.25 % \n",
      "Error of new classifier: 25.50 %\n",
      "\n",
      "\n",
      "==== Iteration 4 ====\n",
      "Error of spherical bayes: 0.25 % \n",
      "Error of new classifier: 25.00 %\n",
      "\n",
      "\n",
      "==== Iteration 5 ====\n",
      "Error of spherical bayes: 0.75 % \n",
      "Error of new classifier: 27.00 %\n",
      "\n",
      "\n",
      "====== 5-fold cross validation errors: ======\n",
      "Average error of spherical bayes: 0.50 % \n",
      "Average error of new classifier: 25.35 %\n"
     ]
    }
   ],
   "source": [
    "# Task b),c) and d)\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Load the data\n",
    "data = genfromtxt('dataset2.txt', delimiter=',')\n",
    "labels = data[:,-1]\n",
    "\n",
    "# Returns the Most likely estimator for mu and sigma based on the given data and class\n",
    "# dimensions is the number of dimensions upon the data \n",
    "def mle_values(data, classifier, dimensions=3):\n",
    "    n=0\n",
    "    \n",
    "    sum_mu  = np.zeros(dimensions) # The same amount of mu values as dimensions\n",
    "    ## Calculate the mu value for the data with class == classifier\n",
    "    for row in data:\n",
    "        if(row[dimensions] == classifier):  # The class of the data is located in the column after the data\n",
    "            for i in range (0,dimensions):  \n",
    "                sum_mu[i] += row[i]         # add the data to the sum\n",
    "            n += 1\n",
    "    mu = sum_mu / float(n)            # Divide the sum by the amount of points to get the mu values\n",
    "    \n",
    "    sum_sig = 0\n",
    "    ## Calculate the sigma value for the data with class == classifier\n",
    "    for row in data:\n",
    "        if(row[dimensions] == classifier):\n",
    "            sum_sig += np.dot(np.transpose(row[0:3] - mu), row[0:3]-mu)  \n",
    "    sig = np.sqrt(sum_sig) / float(n)\n",
    "       \n",
    "    # Return the MLE values for the given data with the given classifier\n",
    "    return mu, sig\n",
    "\n",
    "# Uses the multivariate gaussian pdf to return the probability for a given point with the sigma / mu\n",
    "def sph_gauss(Xtest, mu, sig, dimensions=3):\n",
    "    term1 = (1/ np.sqrt( (2* np.pi)**dimensions * np.sqrt(dimensions)*sig**2))\n",
    "    term2 = np.exp(-1 / (2*sig**2) * np.sum(np.dot(np.transpose(Xtest-mu),Xtest-mu)**2) )\n",
    "    return term1*term2\n",
    "\n",
    "# Predicts a class upon the input Xtest based on the given mus and sigmas\n",
    "def sph_bayes(Xtest, mu_1, sig_1 , mu_2 , sig_2):\n",
    "    \n",
    "    #Calculate the probability for each of the classes (for themselves)\n",
    "    term1 = sph_gauss(Xtest,mu_1, sig_1)\n",
    "    term2 = sph_gauss(Xtest,mu_2, sig_2)\n",
    "    # Tie breaker: (The point seem to be precisely on the decision boundary)\n",
    "    if (term1 == 0 and term2 == 0):\n",
    "            term1 = random.random()\n",
    "            term2 = 1-term1\n",
    "    # Find the actual probability for each of the classes\n",
    "    P1 = term1 / (term1+term2)\n",
    "    P2 = term2 / (term1+ term2)\n",
    "    \n",
    "    # Pick a random value and \"pick\" based on the probabilites\n",
    "    r = random.random()\n",
    "    if (r < P1):\n",
    "        Ytest = 1\n",
    "    else:\n",
    "        Ytest = -1\n",
    "        \n",
    "    # Returns the probabilites for each of the classes and the label that was predicted\n",
    "    return [P1, P2, Ytest]\n",
    "\n",
    "# Predicts a classifier upon the input Xtest based on the given mus\n",
    "def new_classifier(Xtest, mu1, mu2, dimensions=3):\n",
    "    # Follows the formula given in the task\n",
    "    mu = mu1 - mu2\n",
    "    point = Xtest- 1/2 * (mu1+mu2)\n",
    "    dist = np.sqrt(np.sum((mu)**2))\n",
    "    Ytest =np.sign( np.dot(np.transpose(mu), point) / dist)  \n",
    "    \n",
    "    return Ytest\n",
    "\n",
    "# Main\n",
    "def taskd():\n",
    "    print(\"====== 5-fold cross validation ======\")\n",
    "    print(\"\\n\")\n",
    "    KF = model_selection.KFold(n_splits=5, shuffle=True)\n",
    "    x = 1\n",
    "    error_sph = np.zeros(5)\n",
    "    error_new = np.zeros(5)\n",
    "    # Splits the data 5 times into training and test data\n",
    "    for train_index, test_index in KF.split(data):\n",
    "        print(\"==== Iteration %i ====\" % x)\n",
    "        \n",
    "        # Get the mle values for each of the classes for the training data\n",
    "        mu_1, sig_1 = mle_values(data[train_index], 1)\n",
    "        mu_m1, sig_m1 = mle_values(data[train_index], -1)\n",
    "        \n",
    "        bayes = 0\n",
    "        new = 0\n",
    "        n = 0\n",
    "        # Loop through the test data to predict a class using both classifiers\n",
    "        # Also compare the prediction with the actual class\n",
    "        for i in test_index:\n",
    "            n += 1\n",
    "            [_,_,res1] = sph_bayes(data[i][0:3],mu_1, sig_1, mu_m1, sig_m1)\n",
    "            if res1 == data[i][3]:\n",
    "                bayes += 1\n",
    "            res2 = new_classifier(data[i][0:3],mu_1 , mu_m1)\n",
    "            if res2 == data[i][3]:\n",
    "                new += 1\n",
    "        # Present the errors for each iteration\n",
    "        error_sph[x-1] = 100 *(1 - bayes/float(n))\n",
    "        error_new[x-1] = 100 * (1 - new/float(n))\n",
    "        print(\"Error of spherical bayes: %.2f %% \" % error_sph[x-1] )\n",
    "        print(\"Error of new classifier: %.2f %%\" % error_new[x-1] )\n",
    "        print(\"\\n\")\n",
    "        x += 1\n",
    "    print(\"====== 5-fold cross validation errors: ======\")\n",
    "    sphere_5_fold = sum(error_sph) / 5\n",
    "    new_5_fold = sum(error_new) / 5 \n",
    "    print(\"Average error of spherical bayes: %.2f %% \" % sphere_5_fold )\n",
    "    print(\"Average error of new classifier: %.2f %%\" % new_5_fold )\n",
    "                      \n",
    "taskd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [DIGITS dataset classifer, 5 points]\n",
    "\n",
    "Load the DIGITS dataset:\n",
    "```python\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "```\n",
    "This dataset contains $1797$ samples of ten handwritten digit classes. You can further query and visualize the dataset using the various attributes of the returned dictionary:\n",
    "```python\n",
    "data = digits.data\n",
    "print(data.shape)\n",
    "target_names = digits.target_names\n",
    "print (target_names)\n",
    "import matplotlib.pyplot as plt\n",
    "y = digits.target\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "a. Use **new_classifier()** designed previously to do binary classification between classes representing digits \"*5*\" and \"*8*\".\n",
    "\n",
    "b. Investigate an alternative feature function as described below:\n",
    "\n",
    "1. Scale each pixel value to range $[0, 1] $ from original gray-scale ($0-255$). \n",
    "2. Compute variance of each row and column of the image. This will give you a new feature vector of size $16$ i.e. \n",
    "\n",
    "$$ \n",
    "x' = \\left[ \\; Var(row_1)  , Var(row_2), \\ldots , Var(row_{8}), Var(col_1), \\ldots, Var(col_{8}) \\;\\right]^T\n",
    "$$\n",
    "\n",
    "c. Report $5$-fold cross validation results for parts $(a)$ and\n",
    "$(b)$ in a single table. What can you say about the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Iteration 1 ====\n",
      "Accuracy of task a.): 36.111 %\n",
      "Accuracy of task b.): 69.444 %\n",
      "\n",
      "\n",
      "==== Iteration 2 ====\n",
      "Accuracy of task a.): 64.789 %\n",
      "Accuracy of task b.): 67.606 %\n",
      "\n",
      "\n",
      "==== Iteration 3 ====\n",
      "Accuracy of task a.): 70.423 %\n",
      "Accuracy of task b.): 67.606 %\n",
      "\n",
      "\n",
      "==== Iteration 4 ====\n",
      "Accuracy of task a.): 49.296 %\n",
      "Accuracy of task b.): 87.324 %\n",
      "\n",
      "\n",
      "==== Iteration 5 ====\n",
      "Accuracy of task a.): 42.254 %\n",
      "Accuracy of task b.): 76.056 %\n",
      "\n",
      "\n",
      "==== Average accuracy a.) = 52.574 %\n",
      "==== Average accuracy b.) = 73.607 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "data = digits.data\n",
    "target_names = digits.target_names\n",
    "\n",
    "\n",
    "#Modified from question 3\n",
    "def new_classifier(Xtest, mu1, mu2):\n",
    "    d = len(mu1)\n",
    "    \n",
    "    mu = np.zeros(d)\n",
    "    for i in range(0, d):\n",
    "        mu[i] = mu1[i] - mu2[i]\n",
    "        \n",
    "    b = np.zeros(d)\n",
    "    \n",
    "    for i in range(0, d):\n",
    "        b[i] = 0.5 * (mu1[i] + mu2[i])\n",
    "    point = Xtest-b\n",
    "    dist = np.sqrt(np.sum((mu)**2))\n",
    "\n",
    "    Ytest =np.sign( np.dot(np.transpose(mu),point) / dist)\n",
    "\n",
    "    return Ytest\n",
    "\n",
    "\n",
    "####a.)\n",
    "#This function gets the mean of each pixel value in two sets of images and returns it as a 64-float array\n",
    "def get_mean(fives, eights):\n",
    "    mu_fives = np.zeros(64)\n",
    "    for i in range(0, len(fives[0])):\n",
    "        mu_fives[i] = np.mean(fives[:][i])\n",
    "\n",
    "    mu_eights = np.zeros(64)\n",
    "                         \n",
    "    for i in range(0, len(eights[0])):\n",
    "        mu_eights[i] = np.mean(eights[:][i])\n",
    "\n",
    "    return mu_fives, mu_eights\n",
    "\n",
    "\n",
    "####b.)\n",
    "#This function takes two sets of images with pixel values between 0-16 and normalizes them to 0-1,\n",
    "#then finds the variance in each row and column.\n",
    "def get_normalized_variance(fives, eights):\n",
    "    fives = np.asarray(fives)\n",
    "    eights = np.asarray(eights)\n",
    "    fives = fives/16.0\n",
    "    eights = eights/16.0\n",
    "    \n",
    "    #first 8 values are row variance, last 8 values are column variance\n",
    "    sigma_fives = np.zeros(16)\n",
    "    sigma_eights = np.zeros(16)\n",
    "\n",
    "    #Find the feature vector for all images in fives.\n",
    "    for i in range(0, len(fives[0])):\n",
    "        sigma_fives += get_feature_vector(fives[i])\n",
    "\n",
    "    sigma_fives = sigma_fives / len(fives[0])\n",
    "    \n",
    "    for i in range(0, len(eights[0])):\n",
    "        sigma_eights += get_feature_vector(eights[i])\n",
    "\n",
    "    sigma_eights = sigma_eights / len(eights[0])\n",
    "    return sigma_fives, sigma_eights\n",
    "\n",
    "#Takes an array with 64 values, splits in into a matrix with 8x8 columns and finds\n",
    "#the variance over each row and each column, which is returned in an array with 16 elements.\n",
    "#The first 8 elements are the row variances, while the last are the column variances.\n",
    "def get_feature_vector(data):\n",
    "    tmp = np.split(data, 8)\n",
    "    tmp = np.array(tmp)\n",
    "    x = np.zeros(16)\n",
    "    x[:8] += tmp.var(axis = 1)\n",
    "    x[8:] += tmp.var(axis = 0)\n",
    "    return x\n",
    "\n",
    "####c.)\n",
    "fives = data[digits.target == 5]\n",
    "eights = data[digits.target == 8]\n",
    "\n",
    "#Create a model to split the data with, shuffle the values so we\n",
    "#won't get so many consecutive fives and eights.\n",
    "KF = model_selection.KFold(n_splits=5, shuffle=True)\n",
    "k = 1\n",
    "\n",
    "#The total values to be used later\n",
    "total_a = 0.0\n",
    "total_b = 0.0\n",
    "#Merge the fives and eights data\n",
    "data = np.asarray(fives.tolist() + eights.tolist())\n",
    "\n",
    "#Split the data into training and testing data based on the model.and start iterating.\n",
    "for train_index, test_index in KF.split(data):\n",
    "    training_fives = []\n",
    "    training_eights = []\n",
    "\n",
    "    #check every training index.\n",
    "    for i in train_index:\n",
    "        #Check if the index in the training data is a five or an eight.\n",
    "        #The weird boolean is used to compare lists.\n",
    "        if((fives == data[i]).all(1).any()):\n",
    "            training_fives.append(data[i])\n",
    "        else:\n",
    "            training_eights.append(data[i])\n",
    "    \n",
    "    print(\"==== Iteration %i ====\" % k)\n",
    "    k += 1\n",
    "    #from a.)\n",
    "    mu_5, mu_8 = get_mean(training_fives, training_eights)\n",
    "    #from b.)\n",
    "    feature_5, feature_8 = get_normalized_variance(training_fives, training_eights)\n",
    "    \n",
    "    task_a = 0.0\n",
    "    task_b = 0.0\n",
    "    n = 0\n",
    "    for i in test_index:\n",
    "        n += 1\n",
    "        a = new_classifier(data[i], mu_5, mu_8)\n",
    "        #Check f we managed to classify a correct five with the method used in a.)\n",
    "        if ((fives == data[i]).all(1).any() and a ==1):\n",
    "            task_a += 1\n",
    "        #Else, check if it was a correctly classified eight\n",
    "        elif ((eights == data[i]).all(1).any() and a == -1):\n",
    "            task_a += 1\n",
    "        #Get the feature vector for the image\n",
    "        x = get_feature_vector(data[i])\n",
    "        b = new_classifier(x, feature_5, feature_8)\n",
    "        #Check f we managed to classify a correct five with the method used in b.)\n",
    "        if((fives == data[i]).all(1).any() and b == 1):\n",
    "            task_b +=1\n",
    "        #Else, check if it was a correctly classified eight\n",
    "        elif ((eights == data[i]).all(1).any() and b == -1):\n",
    "            task_b += 1\n",
    "    \n",
    "    print(\"Accuracy of task a.): %.3f %%\" %(100 * task_a/float(n)) )\n",
    "    print(\"Accuracy of task b.): %.3f %%\" %(100 * task_b/float(n)) )\n",
    "    print(\"\\n\")\n",
    "    total_a += (task_a/float(n))\n",
    "    total_b += (task_b/float(n))\n",
    "\n",
    "total_a = total_a/5\n",
    "total_b = total_b/5\n",
    "print(\"==== Average accuracy a.) = %.3f %%\" % (total_a*100))\n",
    "print(\"==== Average accuracy b.) = %.3f %%\" % (total_b*100))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "From the results we can see that the classifier using features is producing more reliable results than the classifier using means. The classifier using features usually does so with an accuracy of 70-80%, while the accuracy of the mean classifier can shift from 30% to 90% per iteration. This is because the classifier in task a can't distinguish between fives or eights very well, and will continuously guess that most images are fives. This means that when a subset of the data containing mostly fives is selected, the mean classifier performs ''well'', while when the subset is containing mostly eights it performs poorly. This also means that the classifier using features is more stable since it gives a similar performance regardless of what subset of the data it is given.\n",
    "\n",
    "|     | 1  | 2  | 3  | 4  | 5  | Average |\n",
    "|-----|----|----|----|----|----|---------|\n",
    "| a.) | 36.11% | 64.79% | 70.42% | 49.30% | 42.25% | 52.57%      |\n",
    "| b.) | 69.44% | 67.61% | 67.61% | 67.60% | 87.32% | 73.61%      |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
